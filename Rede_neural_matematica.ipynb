{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Machine Learning</font>\n",
    "\n",
    "# <font color='blue'>Rede Neurais Artificiais</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vers√£o da Linguagem Python\n",
    "from platform import python_version\n",
    "print('Vers√£o da Linguagem Python Usada Neste Jupyter Notebook:', python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Matem√°tica das Redes Neurais Artificiais\n",
    "\n",
    "Este notebook tem como objetivo demonstrar a implementa√ß√£o de uma rede neural do zero, utilizando apenas conceitos matem√°ticos e a biblioteca NumPy. \n",
    "\n",
    "O foco est√° em entender os fundamentos de redes neurais sem depender de frameworks avan√ßados como TensorFlow ou PyTorch.\n",
    "\n",
    "Vamos percorrer desde a inicializa√ß√£o dos pesos at√© o treinamento e avalia√ß√£o da rede. Ao final, voc√™ ter√° uma compreens√£o clara dos c√°lculos internos de uma rede neural.\n",
    "\n",
    "### Construindo a Rede Neural com Programa√ß√£o e Matem√°tica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teremos 2 Partes:\n",
    "\n",
    "- Parte 1 - Vamos construir uma rede neural artificial somente com opera√ß√µes matem√°ticas\n",
    "- Parte 2 - Vamos treinar a rede para Prever a Ocorr√™ncia de C√¢ncer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Arquitetura de Redes Neurais Artificiais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma rede neural t√≠pica √© constitu√≠da por um conjunto de neur√¥nios interligados, infuenciando uns aos outros formando um sistema maior, capaz de armazenar conhecimento adquirido por meio de exemplos apresentados e, assim, podendo realizar infer√™ncias sobre novos conjuntos de dados. Vejamos a arquitetura de redes neurais artificiais.\n",
    "\n",
    "As redes neurais s√£o comumente apresentadas como um grafo orientado, onde os v√©rtices s√£o os neur√¥nios e as arestas as sinapses. A dire√ß√£o das arestas informa o tipo de alimenta√ß√£o, ou seja, como os neur√¥nios s√£o alimentados (recebem sinais de entrada). As redes neurais derivam seu poder devido a sua estrutura massiva e paralela e a habilidade de aprender por experi√™ncia. Essa experi√™ncia √© transmitida por meio de exemplos obtidos do mundo real, definidos como um conjunto de caracter√≠sticas formados por dados de entrada e de sa√≠da. Se apresentamos esses dados de entrada e sa√≠da √† rede, estamos diante de aprendizagem supervsionada e caso apresentemos apenas os dados de entrada, estamos diante de aprendizagem n√£o supervisionada!\n",
    "\n",
    "O conhecimento obtido pela rede atrav√©s dos exemplos √© armazenado na forma de pesos das conex√µes, os quais ser√£o ajustados a fim de tomar decis√µes corretas a partir de novas entradas, ou seja, novas situa√ß√µes do mundo real n√£o conhecidas pela rede. O processo de ajuste dos pesos sinapticos √© realizado pelo algoritmo de aprendizagem, respons√°vel em armazenar na rede o conhecimento do mundo real obtido atraves de exemplos. Existem v√°rios algoritmos de aprendizagem, dentre eles o backpropagation que √© o algoritmo mais utilizado.\n",
    "\n",
    "![title](nnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando os Pacotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por enquanto precisaremos somente do NumPy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vers√µes dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Jeferson Oliveira\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1 - Implementando Uma Rede Neural Artificial Somente com F√≥rmulas Matem√°ticas (Sem Frameworks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fa√ßa a leitura do manual em pdf no pr√≥ximo item de aprendizagem: Par√¢metros x Hiperpar√¢metros.\n",
    "\n",
    "O material esta em ingles, mas √© um material rapido sobre a matematica envolvida no algoritmo de rede neural\n",
    "\n",
    "https://arxiv.org/pdf/1905.07490.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1A - Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](nn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo a Fun√ß√£o Para Inicializa√ß√£o de Pesos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para que a rede neural aprenda padr√µes nos dados, precisamos inicializar os pesos de maneira aleat√≥ria e atribuir valores aos bias. \n",
    "\n",
    "Isso evita que todos os neur√¥nios comecem com os mesmos valores e garante que o aprendizado ocorra de forma eficaz.\n",
    "\n",
    "Utilizaremos uma distribui√ß√£o normal para os pesos, pois isso ajuda na estabilidade do treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para inicializa√ß√£o rand√¥mica dos par√¢metros do modelo\n",
    "def inicializa_parametros(dims_camada_entrada):\n",
    "    \n",
    "    # Dicion√°rio para os par√¢metros\n",
    "    parameters = {}\n",
    "    \n",
    "    # Comprimento das dimens√µes das camadas\n",
    "    comp = len(dims_camada_entrada)\n",
    "    \n",
    "    # Loop pelo comprimento\n",
    "    for i in range(1, comp):\n",
    "        \n",
    "        # Inicializa√ß√£o da matriz de pesos\n",
    "        parameters[\"W\" + str(i)] = np.random.randn(dims_camada_entrada[i], dims_camada_entrada[i - 1]) * 0.01\n",
    "        \n",
    "        # Inicializa√ß√£o do bias\n",
    "        parameters[\"b\" + str(i)] = np.zeros((dims_camada_entrada[i], 1))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo a Fun√ß√£o Sigm√≥ide\n",
    "\n",
    "A principal raz√£o pela qual usamos a fun√ß√£o sigm√≥ide √© porque ela permite converter n√∫meros para valores entre 0 e 1. \n",
    "\n",
    "Portanto, √© especialmente usada para modelos em que temos que prever a probabilidade como uma sa√≠da. Como a probabilidade de qualquer coisa existir apenas entre o intervalo de 0 e 1, sigmoide √© a escolha certa. Algumas caracter√≠siticas da fun√ß√£o sigm√≥ide:\n",
    "\n",
    "- A fun√ß√£o √© diferenci√°vel. Isso significa que podemos encontrar a inclina√ß√£o da curva sigm√≥ide em dois pontos.\n",
    "- A fun√ß√£o sigm√≥ide log√≠stica pode fazer com que uma rede neural fique presa no momento do treinamento.\n",
    "- A fun√ß√£o softmax √© uma fun√ß√£o de ativa√ß√£o log√≠stica mais generalizada, utilizada para a classifica√ß√£o em v√°rias classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se a fun√ß√£o parecer muito abstrata ou estranha para voc√™, n√£o se preocupe muito com detalhes como o n√∫mero de Euler e ou como algu√©m criou essa fun√ß√£o. Para aqueles que n√£o s√£o conhecedores de matem√°tica, a √∫nica coisa importante sobre a fun√ß√£o sigm√≥ide √© primeiro, sua curva e, segundo, sua derivada. Aqui est√£o mais alguns detalhes:\n",
    "\n",
    "- **A fun√ß√£o sigm√≥ide produz resultados semelhantes aos da fun√ß√£o de passo (Step Function) em que a sa√≠da est√° entre 0 e 1. A curva cruza 0,5 a z = 0, e podemos definir regras para a fun√ß√£o de ativa√ß√£o, como: Se a sa√≠da do neur√¥nio sigm√≥ide for maior que ou igual a 0,5, gera 1; se a sa√≠da for menor que 0,5, gera 0.**\n",
    "\n",
    "\n",
    "- A fun√ß√£o sigm√≥ide √© suave e possui uma derivada simples de œÉ(z) * (1 - œÉ (z)), que √© diferenci√°vel em qualquer lugar da curva. \n",
    "\n",
    "\n",
    "- Se z for muito negativo, a sa√≠da ser√° aproximadamente 0; se z for muito positivo, a sa√≠da √© aproximadamente 1; mas em torno de z = 0, onde z n√£o √© muito grande nem muito pequeno, temos um desvio relativamente maior √† medida que z muda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Afinal, O Que √© Derivada?**\n",
    "\n",
    "![title](derivada.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No C√°lculo, a derivada em um ponto de uma fun√ß√£o y = f(x) representa a taxa de varia√ß√£o instant√¢nea de y em rela√ß√£o a x neste ponto. \n",
    "\n",
    "Um exemplo t√≠pico √© a fun√ß√£o velocidade que representa a taxa de varia√ß√£o (derivada) da fun√ß√£o espa√ßo. Do mesmo modo, a fun√ß√£o acelera√ß√£o √© a derivada da fun√ß√£o velocidade. Geometricamente, a derivada no ponto x = a de y = f(x) representa a inclina√ß√£o da reta tangente ao gr√°fico desta fun√ß√£o no ponto (a, f(a)).\n",
    "\n",
    "A fun√ß√£o que a cada ponto x associa a derivada neste ponto de f(x) √© chamada de fun√ß√£o derivada de f(x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](derivada.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em cada ponto, a derivada de f(x) √© a tangente do √¢ngulo que a reta tangente √† curva faz em rela√ß√£o ao eixo das abscissas. A reta √© sempre tangente √† curva azul; a tangente do √¢ngulo que ela faz com o eixo das abscissas √© a derivada. Note-se que a derivada √© positiva quando verde, negativa quando vermelha, e zero quando preta.\n",
    "\n",
    "A derivada de uma fun√ß√£o y = f(x) num ponto x = x0, √© igual ao valor da tangente trigonom√©trica do √¢ngulo formado pela tangente geom√©trica √† curva representativa de y=f(x), no ponto x = x0, ou seja, a derivada √© o coeficiente angular da reta tangente ao gr√°fico da fun√ß√£o no ponto x0.\n",
    "\n",
    "A fun√ß√£o derivada √© representada por f'(x)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o sigm√≥ide\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo a Fun√ß√£o ReLU\n",
    "\n",
    "Para usar a descida de gradiente estoc√°stico com retropropaga√ß√£o de erros para treinar redes neurais profundas, √© necess√°ria uma fun√ß√£o de ativa√ß√£o que se assemelhe e atue como uma fun√ß√£o linear, mas √©, de fato, uma fun√ß√£o n√£o linear que permite que relacionamentos complexos nos dados sejam aprendidos.\n",
    "\n",
    "A solu√ß√£o √© usar a fun√ß√£o de ativa√ß√£o linear retificada ou ReL para abreviar. Um n√≥ ou unidade que implementa essa fun√ß√£o de ativa√ß√£o √© chamado de unidade de ativa√ß√£o linear retificada ou ReLU, para abreviar. Frequentemente, as redes que usam a fun√ß√£o retificadora para as camadas ocultas s√£o chamadas de redes retificadas.\n",
    "\n",
    "A fun√ß√£o ReLU √© definida como ùëì(ùë•) = max (0, ùë•). Normalmente, ela √© aplicada elemento a elemento √† sa√≠da de alguma outra fun√ß√£o, como um produto de vetor e matriz. \n",
    "\n",
    "A ado√ß√£o da ReLU pode ser facilmente considerada um dos marcos na revolu√ß√£o do aprendizado profundo, por ex. as t√©cnicas que agora permitem o desenvolvimento rotineiro de redes neurais muito profundas.\n",
    "\n",
    "A derivada da fun√ß√£o linear retificada tamb√©m √© f√°cil de calcular. **A derivada da fun√ß√£o de ativa√ß√£o √© necess√°ria ao atualizar os pesos de um n√≥ como parte da retropropaga√ß√£o de erro.**\n",
    "\n",
    "A derivada da fun√ß√£o √© a inclina√ß√£o. A inclina√ß√£o para valores negativos √© 0,0 e a inclina√ß√£o para valores positivos √© 1,0.\n",
    "\n",
    "Tradicionalmente, o campo das redes neurais evitou qualquer fun√ß√£o de ativa√ß√£o que n√£o fosse completamente diferenci√°vel, talvez adiando a ado√ß√£o da fun√ß√£o linear retificada e de outras fun√ß√µes lineares. Tecnicamente, n√£o podemos calcular a derivada quando a entrada √© 0,0; portanto, podemos assumir que √© zero. Este n√£o √© um problema na pr√°tica.\n",
    "\n",
    "Os gradientes das ativa√ß√µes tangentes e hiperb√≥licas s√£o menores que a por√ß√£o positiva da ReLU. Isso significa que a parte positiva √© atualizada mais rapidamente √† medida que o treinamento avan√ßa. No entanto, isso tem um custo. O gradiente 0 no lado esquerdo tem seu pr√≥prio problema, chamado \"neur√¥nios mortos\", no qual uma atualiza√ß√£o de gradiente define os valores recebidos para uma ReLU, de modo que a sa√≠da √© sempre zero; unidades ReLU modificadas, como ELU (ou Leaky ReLU, ou PReLU, etc.) podem melhorar isso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](relu.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de ativa√ß√£o ReLu (Rectified Linear Unit)\n",
    "def relu(Z):\n",
    "    A = abs(Z * (Z > 0))\n",
    "    return A, Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](net-relu.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo a Ativa√ß√£o Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opera√ß√£o de ativa√ß√£o\n",
    "# A √© a matriz com os dados de entrada\n",
    "# W √© a matriz de pesos\n",
    "# b √© o bias\n",
    "def linear_activation(A, W, b):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construindo o Processo de Forward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No forward propagation, os dados percorrem a rede desde a entrada at√© a sa√≠da final. Esse processo envolve o c√°lculo de ativa√ß√µes para cada camada oculta utilizando uma fun√ß√£o de ativa√ß√£o. \n",
    "\n",
    "Aqui utilizaremos a fun√ß√£o Sigmoid (ou ReLU, dependendo do caso), pois elas ajudam a modelar rela√ß√µes n√£o lineares entre os dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Movimento para frente (forward)\n",
    "def forward(A_prev, W, b, activation):\n",
    "    \n",
    "    # Se a fun√ß√£o de ativa√ß√£o for Sigmoid, entramos neste bloco\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        \n",
    "    # Se n√£o, se for ReLu, entramos neste bloco    \n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_activation(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinando Ativa√ß√£o e Propaga√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Propaga√ß√£o para frente\n",
    "def forward_propagation(X, parameters):\n",
    "    \n",
    "    # Lista de valores anteriores (cache)\n",
    "    caches = []\n",
    "    \n",
    "    # Dados de entrada\n",
    "    A = X\n",
    "    \n",
    "    # Comprimento dos par√¢metros\n",
    "    L = len(parameters) // 2\n",
    "   \n",
    "    # Loop\n",
    "    for i in range(1, L):\n",
    "      \n",
    "        # Guarda o valor pr√©vio de A\n",
    "        A_prev = A\n",
    "        \n",
    "        # Executa o forward\n",
    "        A, cache = forward(A_prev, parameters[\"W\" + str(i)], parameters[\"b\" + str(i)], activation = \"relu\")\n",
    "        \n",
    "        # Grava o cache\n",
    "        caches.append(cache)\n",
    "    \n",
    "    # Sa√≠da na √∫ltima camada\n",
    "    A_last, cache = forward(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)], activation = \"sigmoid\")\n",
    "    \n",
    "    # Grava o cache\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return(A_last, caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo a Fun√ß√£o de Custo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](custo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de custo (ou fun√ß√£o de erro)\n",
    "def calcula_custo(A_last, Y):\n",
    "    \n",
    "    # Ajusta o shape de Y para obter seu comprimento (total de elementos)\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Calcula o custo comparando valor real e previso\n",
    "    custo = (-1 / m) * np.sum((Y * np.log(A_last)) + ((1 - Y) * np.log(1 - A_last)))\n",
    "    \n",
    "    # Ajusta o shape do custo\n",
    "    custo = np.squeeze(custo)\n",
    "    \n",
    "    return(custo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 1B - Backward Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O backpropagation √© o cora√ß√£o do aprendizado da rede neural. Ele calcula o gradiente da fun√ß√£o de perda em rela√ß√£o aos pesos e os ajusta utilizando o m√©todo do gradiente descendente. \n",
    "\n",
    "Esse processo garante que a rede aprenda padr√µes a cada itera√ß√£o, reduzindo gradativamente o erro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](backpropagation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo o Backward Propagation - Fun√ß√£o Sigm√≥ide Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o sigmoid para o backpropagation \n",
    "# Fazemos o c√°lculo da derivada pois n√£o queremos o valor completo da fun√ß√£o, mas sim sua varia√ß√£o\n",
    "def sigmoid_backward(da, Z):\n",
    "    \n",
    "    # Calculamos a derivada de Z\n",
    "    dg = (1 / (1 + np.exp(-Z))) * (1 - (1 / (1 + np.exp(-Z))))\n",
    "    \n",
    "    # Encontramos a mudan√ßa na derivada de z\n",
    "    dz = da * dg\n",
    "    return dz\n",
    "\n",
    "# Compare com a fun√ß√£o sigmoid do forward propagation\n",
    "# A = 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo o Backward Propagation - Fun√ß√£o ReLu Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o relu para o backpropagation \n",
    "# Fazemos o c√°lculo da derivada pois n√£o queremos o valor completo da fun√ß√£o, mas sim sua varia√ß√£o\n",
    "def relu_backward(da, Z):\n",
    "    \n",
    "    dg = 1 * ( Z >= 0)\n",
    "    dz = da * dg\n",
    "    return dz\n",
    "\n",
    "# Compare com a fun√ß√£o relu do forward propagation:\n",
    "# A = abs(Z * (Z > 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo o Backward Propagation - Ativa√ß√£o Linear Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ativa√ß√£o linear para o backpropagation\n",
    "def linear_backward_function(dz, cache):\n",
    "    \n",
    "    # Recebe os valores do cache (mem√≥ria)\n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    # Shape de m\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    # Calcula a derivada de W (resultado da opera√ß√£o com dz)\n",
    "    dW = (1 / m) * np.dot(dz, A_prev.T)\n",
    "    \n",
    "    # Calcula a derivada de b (resultado da opera√ß√£o com dz)\n",
    "    db = (1 / m) * np.sum(dz, axis = 1, keepdims = True)\n",
    "    \n",
    "    # Calcula a derivada da opera√ß√£o\n",
    "    dA_prev = np.dot(W.T, dz)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desenvolvendo o Backward Propagation - Ativa√ß√£o Linear Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o que define o tipo de ativa√ß√£o (relu ou sigmoid)\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \n",
    "    # Extrai o cache\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    # Verifica se a ativa√ß√£o √© relu\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)\n",
    "        \n",
    "    # Verifica se a ativa√ß√£o √© sigmoid\n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward_function(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combinando Ativa√ß√£o e Retropropaga√ß√£o - Algoritmo Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algoritmo Backpropagation (calcula os gradientes para atualiza√ß√£o dos pesos)\n",
    "# AL = Valor previsto no Forward\n",
    "# Y = Valor real\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    \n",
    "    # Dicion√°rio para os gradientes\n",
    "    grads = {}\n",
    "    \n",
    "    # Comprimento dos dados (que est√£o no cache)\n",
    "    L = len(caches)\n",
    "    \n",
    "    # Extrai o comprimento para o valor de m\n",
    "    m = AL.shape[1]\n",
    "    \n",
    "    # Ajusta o shape de Y\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # Calcula a derivada da previs√£o final da rede (feita ao final do Forward Propagation)\n",
    "    dAL = -((Y / AL) - ((1 - Y) / (1 - AL)))\n",
    "    \n",
    "    # Captura o valor corrente do cache\n",
    "    current_cache = caches[L - 1]\n",
    "    \n",
    "    # Gera a lista de gradiente para os dados, os pesos e o bias\n",
    "    # Fazemos isso uma vez, pois estamos na parte final da rede, iniciando o caminho de volta\n",
    "    grads[\"dA\" + str(L - 1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    \n",
    "    # Loop para calcular a derivada durante as ativa√ß√µes lineares com a relu\n",
    "    for l in reversed(range(L - 1)):\n",
    "        \n",
    "        # Cache atual\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        # Calcula as derivadas\n",
    "        dA_prev, dW, db = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, activation = \"relu\")\n",
    "        \n",
    "        # Alimenta os gradientes na lista, usando o √≠ndice respectivo\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] = dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradientes e Atualiza√ß√£o dos Pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o de atualiza√ß√£o de pesos\n",
    "def atualiza_pesos(parameters, grads, learning_rate):\n",
    "    \n",
    "    # Comprimento da estrutura de dados com os par√¢metros (pesos e bias)\n",
    "    L = len(parameters)//2\n",
    "    \n",
    "    # Loop para atualiza√ß√£o dos pesos\n",
    "    for l in range(L):\n",
    "        \n",
    "        # Atualiza√ß√£o dos pesos\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - (learning_rate * grads[\"dW\" + str(l + 1)])\n",
    "        \n",
    "        # Atualiza√ß√£o do bias\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - (learning_rate * grads[\"db\" + str(l + 1)])\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementando a Rede Completa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo completo da rede neural\n",
    "def modeloNN(X, Y, dims_camada_entrada, learning_rate = 0.0075, num_iterations = 100):\n",
    "    \n",
    "    # Lista para receber o custo a cada √©poca de treinamento\n",
    "    custos = []\n",
    "    \n",
    "    # Inicializa os par√¢metros\n",
    "    parametros = inicializa_parametros(dims_camada_entrada)\n",
    "    \n",
    "    # Loop pelo n√∫mero de itera√ß√µes (√©pocas)\n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Forward Propagation\n",
    "        AL, caches = forward_propagation(X, parametros)\n",
    "        \n",
    "        # Calcula o custo\n",
    "        custo = calcula_custo(AL, Y)\n",
    "        \n",
    "        # Backward Propagation\n",
    "        # Nota: ao inv√©s de AL e Y, poder√≠amos passar somente o valor do custo\n",
    "        # Estamos passando o valor de AL e Y para fique claro didaticamente o que est√° sendo feito\n",
    "        gradientes = backward_propagation(AL, Y, caches)\n",
    "        \n",
    "        # Atualiza os pesos\n",
    "        parametros = atualiza_pesos(parametros, gradientes, learning_rate)\n",
    "        \n",
    "        # Print do valor intermedi√°rio do custo\n",
    "        # A redu√ß√£o do custo indica o aprendizado do modelo\n",
    "        if i % 10 == 0:\n",
    "            print(\"Custo Ap√≥s \" + str(i) + \" itera√ß√µes √© \" + str(custo))\n",
    "            custos.append(custo)\n",
    "            \n",
    "    return parametros, custos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fun√ß√£o para fazer as previs√µes\n",
    "# N√£o precisamos do Backpropagation pois ao fazer previs√µes como o modelo treinado, \n",
    "# teremos os melhores valores de pesos (parametros)\n",
    "def predict(X, parametros):\n",
    "    AL, caches = forward_propagation(X, parametros)\n",
    "    return AL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2 - Vamos treinar a rede para Prever a Ocorr√™ncia de C√¢ncer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Projeto 4 - Usando a Rede Neural Para Prever a Ocorr√™ncia de C√¢ncer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vers√µes dos pacotes usados neste jupyter notebook\n",
    "%reload_ext watermark\n",
    "%watermark -a \"Jeferson Oliveira\" --iversions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os Dados\n",
    "\n",
    "https://scikit-learn.org/stable/datasets/index.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamos o objeto completo\n",
    "temp = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tipo do objeto\n",
    "type(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza o objeto\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregamos o dataset\n",
    "dados = pd.DataFrame(columns = load_breast_cancer()[\"feature_names\"], data = load_breast_cancer()[\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape\n",
    "dados.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os dados\n",
    "dados.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verifica se temos valores ausentes\n",
    "dados.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separa a vari√°vel target\n",
    "target = load_breast_cancer()[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza a vari√°vel\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de registros por classe - C√¢ncer Benigno\n",
    "np.count_nonzero(target == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total de registros por classe - C√¢ncer Maligno\n",
    "np.count_nonzero(target == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos extrair os labels\n",
    "\n",
    "# Dicion√°rio para os labels\n",
    "labels = {}\n",
    "\n",
    "# Nomes das classes da vari√°vel target\n",
    "target_names = load_breast_cancer()[\"target_names\"]\n",
    "\n",
    "# Mapeamento\n",
    "for i in range(len(target_names)):\n",
    "    labels.update({i:target_names[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os labels\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agora preparamos as vari√°veis preditoras em X\n",
    "X = np.array(dados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os dados de entrada\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos os dados de entrada e sa√≠da em treino e teste\n",
    "X_treino, X_teste, y_treino, y_teste = train_test_split(X, target, test_size = 0.15, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape dos dados de treino\n",
    "print(X_treino.shape)\n",
    "print(y_treino.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shape dos dados de teste\n",
    "print(X_teste.shape)\n",
    "print(y_teste.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajusta o shape dos dados de entrada\n",
    "X_treino = X_treino.T\n",
    "X_teste = X_teste.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_treino.shape)\n",
    "print(X_teste.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precisamos ajustar tamb√©m os dados de sa√≠da\n",
    "y_treino = y_treino.reshape(1, len(y_treino))\n",
    "y_teste = y_teste.reshape(1, len(y_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_treino.shape)\n",
    "print(y_teste.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vari√°vel com as dimens√µes de entrada para oo n√∫mero de neur√¥nios \n",
    "dims_camada_entrada = [X_treino.shape[0], 50, 20, 5, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims_camada_entrada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ap√≥s definir todas as fun√ß√µes da rede, precisamos trein√°-la. Isso envolve iterar v√°rias vezes sobre os dados, ajustando os pesos para minimizar a fun√ß√£o de perda. \n",
    "\n",
    "Durante a avalia√ß√£o, aplicamos os dados de teste para verificar se a rede generalizou bem ou se est√° apenas decorando os exemplos de treino."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinamento do modelo\n",
    "\n",
    "print(\"\\nIniciando o Treinamento.\\n\")\n",
    "\n",
    "parametros, custo = modeloNN(X = X_treino, \n",
    "                             Y = y_treino, \n",
    "                             dims_camada_entrada = dims_camada_entrada, \n",
    "                             num_iterations = 3000, \n",
    "                             learning_rate = 0.0075)\n",
    "\n",
    "print(\"\\nTreinamento Conclu√≠do.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot do erro durante o treinamento\n",
    "plt.plot(custo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes com os dados de treino\n",
    "y_pred_treino = predict(X_treino, parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza as previs√µes\n",
    "y_pred_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos o shape em treino\n",
    "y_pred_treino = y_pred_treino.reshape(-1)\n",
    "y_treino = y_treino.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_treino > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertemos as previs√µes para o valor bin√°rio de classe \n",
    "# (0 ou 1, usando como threshold o valor de 0.5 da probabilidade)\n",
    "y_pred_treino = 1 * (y_pred_treino > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos a acur√°cia comparando valor real com valor previsto\n",
    "acc_treino = sum(1 * (y_pred_treino == y_treino)) / len(y_pred_treino) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Acur√°cia nos dados de treino: \" + str(acc_treino))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_treino, y_pred_treino, target_names = ['Maligno', 'Benigno']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previs√µes com o modelo usando dados de teste\n",
    "y_pred_teste = predict(X_teste, parametros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os dados\n",
    "y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustamos os shapes\n",
    "y_pred_teste = y_pred_teste.reshape(-1)\n",
    "y_teste = y_teste.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertemos as previs√µes para o valor bin√°rio de classe\n",
    "y_pred_teste = 1 * (y_pred_teste > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizamos as previs√µes\n",
    "y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos a acur√°cia\n",
    "acuracia = sum(1 * (y_pred_teste == y_teste)) / len(y_pred_teste) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Acur√°cia nos dados de teste: \" + str(acuracia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_teste, y_pred_teste, target_names = ['Maligno', 'Benigno']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclus√£o\n",
    "\n",
    "Neste notebook, implementamos uma rede neural do zero utilizando apenas opera√ß√µes matem√°ticas e a biblioteca NumPy. Passamos por todas as etapas fundamentais do funcionamento de uma rede neural, incluindo a inicializa√ß√£o dos pesos, forward propagation, c√°lculo da fun√ß√£o de perda, backpropagation e atualiza√ß√£o dos pesos com gradiente descendente.\n",
    "\n",
    "Ao construir a rede manualmente, conseguimos visualizar como cada etapa contribui para o aprendizado do modelo, oferecendo uma compreens√£o mais profunda do que acontece por tr√°s dos frameworks populares como TensorFlow e PyTorch.\n",
    "\n",
    "### Pr√≥ximos Passos\n",
    "Se quiser expandir este projeto, aqui est√£o algumas sugest√µes:\n",
    "\n",
    "- Implementar outras fun√ß√µes de ativa√ß√£o, como Tanh ou Leaky ReLU.\n",
    "- Testar diferentes fun√ß√µes de perda para problemas espec√≠ficos.\n",
    "- Adicionar suporte para m√∫ltiplas camadas ocultas e diferentes quantidades de neur√¥nios.\n",
    "- Explorar t√©cnicas de otimiza√ß√£o mais avan√ßadas, como Adam ou RMSprop.\n",
    "\n",
    "Essa abordagem did√°tica permite uma base s√≥lida para quem deseja avan√ßar no estudo de redes neurais e machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fim"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
